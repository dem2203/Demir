# main.py
"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš€ DEMIR AI v7.0 - ENTERPRISE MASTER ORCHESTRATOR
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ULTRA PROFESSIONAL AI TRADING SYSTEM - NO COMPROMISES

ARCHITECTURE:
    âœ… 50+ Layer Signal Generation (Technical, Sentiment, ML, On-Chain, Risk)
    âœ… Real-time WebSocket + REST API Hybrid
    âœ… Multi-Exchange Price Verification (Binance, Bybit, Coinbase)
    âœ… PostgreSQL with Connection Pooling
    âœ… Circuit Breaker Pattern for API Calls
    âœ… Distributed Task Queue (Celery-like)
    âœ… Advanced Monitoring & Alerting
    âœ… AI Self-Learning System
    âœ… Advisory Mode (NO AUTO-TRADING)
    âœ… Production-Grade Error Handling
    âœ… Zero-Downtime Deployment Ready

DATA INTEGRITY:
    âŒ ZERO Mock Data
    âŒ ZERO Fake Data
    âŒ ZERO Test Data
    âŒ ZERO Fallback Data
    âŒ ZERO Hardcoded Data
    âœ… 100% Real Exchange Data Only

DEPLOYMENT:
    - Railway Production Environment
    - GitHub CI/CD Integration
    - Docker Container Support
    - Kubernetes Ready
    - Auto-Scaling Enabled

AUTHOR: DEMIR AI Research Team
VERSION: 7.0
DATE: 2025-11-19
LICENSE: Proprietary
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

# ============================================================================
# IMPORTS
# ============================================================================

import os
import sys
import time
import json
import signal
import asyncio
import logging
import threading
import traceback
import requests  # For Telegram and HTTP requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from collections import defaultdict, deque
from functools import wraps
import queue

# Third-party
import pytz
import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2.pool import ThreadedConnectionPool
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from werkzeug.middleware.proxy_fix import ProxyFix
from dotenv import load_dotenv
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler

# Add project root to path
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)

# Load environment variables
load_dotenv()

# Project imports
from config import (
    ADVISORY_MODE,
    DEFAULT_TRACKED_SYMBOLS,
    OPPORTUNITY_THRESHOLDS,
    BINANCE_API_KEY,
    BINANCE_API_SECRET,
    DATABASE_URL,
    TELEGRAM_TOKEN,
    TELEGRAM_CHAT_ID
)

# Advanced AI Components
from advanced_ai.signal_engine_integration import SignalGroupOrchestrator
from advanced_ai.opportunity_engine import OpportunityEngine, TradePlan
from advanced_ai.advisor_core import AdvisorCore
from advanced_ai.regime_detector import MarketRegimeDetector
from advanced_ai.layer_optimizer import LayerOptimizer
try:
    from advanced_ai.causal_reasoning import CausalReasoningEngine
    print("âœ… CausalReasoningEngine loaded")
except (ImportError, ModuleNotFoundError, SyntaxError) as e:
    print(f"âš ï¸  CausalReasoningEngine skipped: {e}")
    CausalReasoningEngine = None

# Analytics Components
from analytics.advisor_opportunity_service import AdvisorOpportunityService
from analytics.performance_engine import PerformanceEngine
try:
    from analytics.backtest_engine_production import BacktestEngine
    print("âœ… BacktestEngine loaded")
except (ImportError, ModuleNotFoundError, SyntaxError) as e:
    print(f"âš ï¸  BacktestEngine skipped: {e}")
    BacktestEngine = None
from analytics.trade_analyzer import TradeAnalyzer
from analytics.report_generator import ReportGenerator

# Integration Components
from integrations.binance_websocket_v3 import BinanceWebSocketManager
from integrations.multi_exchange_api import MultiExchangeDataFetcher
from integrations.market_intelligence import MarketIntelligenceAggregator
from integrations.sentiment_aggregator import SentimentAggregator
from integrations.macro_data_aggregator import MacroDataAggregator

# Utility Components
from utils.logger_setup import setup_logger
from utils.real_data_verifier_pro import RealDataVerifier
from utils.signal_validator_comprehensive import SignalValidator
from utils.signal_processor_advanced import AdvancedSignalProcessor
from utils.circuit_breaker import CircuitBreaker
from utils.retry_manager import RetryManager
from utils.redis_cache import RedisCache
from utils.health_monitor import HealthMonitor

# UI Components
from ui.data_fetcher_realtime import RealtimeDataFetcher
from ui.telegram_tradeplan_notifier import TelegramTradePlanNotifier
from ui.dashboard_backend import EnterpriseDashboard

# Database
from database_manager_production import DatabaseManager

# ============================================================================
# ENHANCED LOGGING SYSTEM
# ============================================================================

class ColoredFormatter(logging.Formatter):
    """Professional colored logging with emoji indicators"""
    
    COLORS = {
        'DEBUG': '\033[36m',    # Cyan
        'INFO': '\033[92m',     # Green
        'WARNING': '\033[93m',  # Yellow
        'ERROR': '\033[91m',    # Red
        'CRITICAL': '\033[95m', # Magenta
        'RESET': '\033[0m'
    }
    
    EMOJI = {
        'DEBUG': 'ğŸ”',
        'INFO': 'âœ…',
        'WARNING': 'âš ï¸',
        'ERROR': 'âŒ',
        'CRITICAL': 'ğŸ”¥'
    }
    
    def format(self, record):
        # Add color
        log_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        emoji = self.EMOJI.get(record.levelname, 'â€¢')
        
        # Format levelname with color and emoji
        record.levelname = f"{emoji} {log_color}[{record.levelname}]{self.COLORS['RESET']}"
        
        return super().format(record)

def setup_production_logging():
    """Setup production-grade logging system"""
    
    # Create logs directory
    log_dir = os.path.join(PROJECT_ROOT, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # Main application log
    app_log = os.path.join(log_dir, 'demir_ai.log')
    error_log = os.path.join(log_dir, 'errors.log')
    performance_log = os.path.join(log_dir, 'performance.log')
    
    # Formatters
    detailed_format = '%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s'
    simple_format = '%(asctime)s - %(levelname)s - %(message)s'
    
    # Console handler (colored)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(ColoredFormatter(detailed_format))
    
    # File handler (rotating daily)
    file_handler = TimedRotatingFileHandler(
        app_log,
        when='midnight',
        interval=1,
        backupCount=30,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(detailed_format))
    
    # Error file handler (errors only)
    error_handler = RotatingFileHandler(
        error_log,
        maxBytes=10*1024*1024,
        backupCount=10,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(logging.Formatter(detailed_format))
    
    # Performance log handler
    perf_handler = RotatingFileHandler(
        performance_log,
        maxBytes=10*1024*1024,
        backupCount=5,
        encoding='utf-8'
    )
    perf_handler.setLevel(logging.INFO)
    perf_handler.setFormatter(logging.Formatter(simple_format))
    
    # Root logger configuration
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(error_handler)
    
    # Performance logger
    perf_logger = logging.getLogger('performance')
    perf_logger.addHandler(perf_handler)
    perf_logger.propagate = False
    
    return logging.getLogger('DEMIR_AI_MASTER')

# Initialize logger
logger = setup_production_logging()

# ============================================================================
# STARTUP BANNER
# ============================================================================

def print_startup_banner():
    """Print professional startup banner"""
    banner = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘                        ğŸ¤– DEMIR AI v7.0 ENTERPRISE                            â•‘
â•‘                                                                               â•‘
â•‘                    Ultra-Professional Trading AI System                       â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  ğŸ—ï¸  Architecture:     Microservices + Event-Driven                          â•‘
â•‘  ğŸ§  AI Layers:         50+ Advanced ML Models                                â•‘
â•‘  ğŸ“Š Data Sources:      Multi-Exchange Real-Time                              â•‘
â•‘  ğŸ”’ Security:          Enterprise-Grade                                      â•‘
â•‘  âš¡ Performance:       High-Frequency Ready                                  â•‘
â•‘  ğŸŒ Deployment:        Railway Production                                    â•‘
â•‘                                                                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  â° Started:           {datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}                        â•‘
â•‘  ğŸŒ Environment:       {os.getenv('ENVIRONMENT', 'production').upper():<15}                          â•‘
â•‘  ğŸ’¼ Mode:              {'ADVISORY (NO AUTO-TRADING)' if ADVISORY_MODE else 'TRADING ENABLED':<15}                          â•‘
â•‘  ğŸ“ˆ Tracked Symbols:   {len(DEFAULT_TRACKED_SYMBOLS):<3} pairs                                      â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)
    logger.info("=" * 100)
    logger.info("DEMIR AI v7.0 ENTERPRISE - INITIALIZING")
    logger.info("=" * 100)

# ============================================================================
# GLOBAL STATE MANAGER
# ============================================================================

class GlobalStateManager:
    """
    Centralized state management for entire application
    Thread-safe, singleton pattern
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self._initialized = True
        
        # System status
        self.is_running = False
        self.start_time = None
        self.shutdown_requested = False
        
        # Component references
        self.db_manager: Optional[DatabaseManager] = None
        self.ws_manager: Optional[BinanceWebSocketManager] = None
        self.data_fetcher: Optional[MultiExchangeDataFetcher] = None
        self.signal_orchestrator: Optional[SignalGroupOrchestrator] = None
        self.opportunity_engine: Optional[OpportunityEngine] = None
        self.advisor_service: Optional[AdvisorOpportunityService] = None
        self.performance_engine: Optional[PerformanceEngine] = None
        self.health_monitor: Optional[HealthMonitor] = None
        self.redis_cache: Optional[RedisCache] = None
        
        # Data validators
        self.real_data_verifier: Optional[RealDataVerifier] = None
        self.signal_validator: Optional[SignalValidator] = None
        
        # Statistics
        self.stats = {
            'total_signals_generated': 0,
            'total_opportunities_found': 0,
            'total_api_calls': 0,
            'total_errors': 0,
            'uptime_seconds': 0
        }
        
        # Thread management
        self.threads: Dict[str, threading.Thread] = {}
        self.thread_status: Dict[str, str] = {}
        
        # Locks
        self.stats_lock = threading.Lock()
        self.component_lock = threading.Lock()
        
        logger.info("âœ… GlobalStateManager initialized")
    
    def increment_stat(self, stat_name: str, amount: int = 1):
        """Thread-safe stat increment"""
        with self.stats_lock:
            self.stats[stat_name] = self.stats.get(stat_name, 0) + amount
    
    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics"""
        with self.stats_lock:
            if self.start_time:
                self.stats['uptime_seconds'] = (datetime.now() - self.start_time).total_seconds()
            return dict(self.stats)
    
    def register_thread(self, name: str, thread: threading.Thread):
        """Register a managed thread"""
        self.threads[name] = thread
        self.thread_status[name] = 'running'
        logger.info(f"ğŸ§µ Thread registered: {name}")
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get comprehensive health status"""
        return {
            'system': {
                'is_running': self.is_running,
                'uptime_seconds': self.get_stats()['uptime_seconds'],
                'shutdown_requested': self.shutdown_requested
            },
            'components': {
                'database': self.db_manager is not None and self.db_manager.is_healthy(),
                'websocket': self.ws_manager is not None and self.ws_manager.is_healthy(),
                'data_fetcher': self.data_fetcher is not None,
                'signal_orchestrator': self.signal_orchestrator is not None,
                'redis_cache': self.redis_cache is not None and self.redis_cache.is_connected()
            },
            'threads': {
                name: {
                    'status': self.thread_status.get(name, 'unknown'),
                    'is_alive': thread.is_alive()
                }
                for name, thread in self.threads.items()
            },
            'stats': self.get_stats()
        }

# Global state instance
state = GlobalStateManager()

# ============================================================================
# DATA INTEGRITY VALIDATORS
# ============================================================================

class EnterpriseDataValidator:
    """
    Enterprise-grade data validation system
    ZERO TOLERANCE for mock/fake/test data
    """
    
    # Forbidden keywords (expanded list)
    FORBIDDEN_KEYWORDS = [
        'mock', 'fake', 'test', 'fallback', 'prototype', 'dummy', 
        'sample', 'hard', 'fixed', 'static', 'demo', 'example',
        'placeholder', 'stub', 'template', 'skeleton'
    ]
    
    def __init__(self):
        self.real_verifier = RealDataVerifier()
        self.validation_history = deque(maxlen=1000)
        self.rejection_count = 0
        
        logger.info("âœ… EnterpriseDataValidator initialized - ZERO TOLERANCE MODE")
    
    def validate_price_data(
        self,
        symbol: str,
        price: float,
        source: str,
        timestamp: Union[float, datetime]
    ) -> Tuple[bool, str]:
        """
        Validate price data with extreme prejudice
        
        Returns:
            (is_valid, reason)
        """
        # 1. Price sanity checks
        if not isinstance(price, (int, float)):
            return False, f"Price is not numeric: {type(price)}"
        
        if price <= 0:
            return False, f"Price is non-positive: {price}"
        
        if price > 1_000_000_000:
            return False, f"Price unreasonably high: {price}"
        
        # 2. Timestamp validation
        if isinstance(timestamp, datetime):
            ts = timestamp.timestamp()
        else:
            ts = float(timestamp)
        
        current_time = time.time()
        age = current_time - ts
        
        if age < 0:
            return False, f"Future timestamp detected: {age}s"
        
        if age > 3600:  # 1 hour max age
            return False, f"Stale data: {age:.0f}s old"
        
        # 3. Source verification
        if source not in ['BINANCE', 'BYBIT', 'COINBASE', 'REAL']:
            return False, f"Unknown source: {source}"
        
        # 4. Check for suspicious keywords in source
        source_lower = source.lower()
        for keyword in self.FORBIDDEN_KEYWORDS:
            if keyword in source_lower:
                return False, f"Forbidden keyword '{keyword}' in source"
        
        # 5. Cross-verify with real verifier
        is_real, msg = self.real_verifier.verify_price(symbol, price)
        if not is_real:
            return False, f"Real verifier rejected: {msg}"
        
        # 6. Check price movement (if we have history)
        if symbol in self.real_verifier.last_prices:
            last_price = self.real_verifier.last_prices[symbol]
            change_pct = abs(price - last_price) / last_price
            
            if change_pct > 0.20:  # 20% movement is suspicious
                logger.warning(
                    f"âš ï¸ SUSPICIOUS: {symbol} moved {change_pct*100:.1f}% "
                    f"from ${last_price:.2f} to ${price:.2f}"
                )
        
        # All checks passed
        self.validation_history.append({
            'symbol': symbol,
            'price': price,
            'source': source,
            'timestamp': ts,
            'validated_at': time.time()
        })
        
        return True, f"Price validated: ${price:.2f} from {source}"
    
    def validate_signal(self, signal: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        Comprehensive signal validation
        
        Returns:
            (is_valid, list_of_issues)
        """
        issues = []
        
        # 1. Required fields
        required_fields = [
            'symbol', 'direction', 'entry_price', 'timestamp', 
            'confidence', 'data_source'
        ]
        
        for field in required_fields:
            if field not in signal:
                issues.append(f"âŒ Missing required field: {field}")
        
        if issues:
            return False, issues
        
        # 2. Check for forbidden keywords
        signal_str = json.dumps(signal).lower()
        for keyword in self.FORBIDDEN_KEYWORDS:
            if keyword in signal_str:
                issues.append(f"âŒ Forbidden keyword '{keyword}' detected in signal")
        
        # 3. Direction validation
        if signal['direction'] not in ['LONG', 'SHORT', 'NEUTRAL']:
            issues.append(f"âŒ Invalid direction: {signal['direction']}")
        
        # 4. Confidence range
        conf = signal.get('confidence', 0)
        if not (0 <= conf <= 1):
            issues.append(f"âŒ Confidence out of range: {conf}")
        
        # 5. Price validation
        entry_price = signal.get('entry_price', 0)
        is_valid_price, price_msg = self.validate_price_data(
            symbol=signal['symbol'],
            price=entry_price,
            source=signal.get('data_source', 'UNKNOWN'),
            timestamp=signal['timestamp']
        )
        
        if not is_valid_price:
            issues.append(f"âŒ Price validation failed: {price_msg}")
        
        # 6. Stop loss / Take profit logic
        direction = signal.get('direction')
        sl = signal.get('sl', 0)
        tp1 = signal.get('tp1', 0)
        
        if direction == 'LONG':
            if sl >= entry_price:
                issues.append(f"âŒ LONG: SL must be < entry ({sl} >= {entry_price})")
            if tp1 <= entry_price:
                issues.append(f"âŒ LONG: TP must be > entry ({tp1} <= {entry_price})")
        
        elif direction == 'SHORT':
            if sl <= entry_price:
                issues.append(f"âŒ SHORT: SL must be > entry ({sl} <= {entry_price})")
            if tp1 >= entry_price:
                issues.append(f"âŒ SHORT: TP must be < entry ({tp1} >= {entry_price})")
        
        # 7. Risk/Reward ratio check
        if sl and tp1:
            risk = abs(entry_price - sl)
            reward = abs(tp1 - entry_price)
            
            if risk > 0:
                rr_ratio = reward / risk
                if rr_ratio < 1.0:
                    issues.append(f"âŒ Poor R:R ratio: {rr_ratio:.2f} < 1.0")
        
        # Final verdict
        if issues:
            self.rejection_count += 1
            logger.error(f"âŒ SIGNAL REJECTED ({self.rejection_count} total): {issues}")
            return False, issues
        
        return True, []
    
    def get_validation_stats(self) -> Dict[str, Any]:
        """Get validation statistics"""
        return {
            'total_validations': len(self.validation_history),
            'rejection_count': self.rejection_count,
            'rejection_rate': self.rejection_count / max(len(self.validation_history), 1),
            'recent_validations': list(self.validation_history)[-10:]
        }

# ============================================================================
# PERFORMANCE MONITOR
# ============================================================================

class PerformanceMonitor:
    """Monitor and profile system performance"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.lock = threading.Lock()
        self.perf_logger = logging.getLogger('performance')
    
    def measure(self, operation: str):
        """Decorator to measure function execution time"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start = time.time()
                try:
                    result = func(*args, **kwargs)
                    duration = time.time() - start
                    
                    with self.lock:
                        self.metrics[operation].append(duration)
                    
                    if duration > 5.0:  # Log slow operations
                        self.perf_logger.warning(
                            f"âš ï¸ SLOW OPERATION: {operation} took {duration:.2f}s"
                        )
                    
                    return result
                except Exception as e:
                    duration = time.time() - start
                    self.perf_logger.error(
                        f"âŒ {operation} failed after {duration:.2f}s: {e}"
                    )
                    raise
            return wrapper
        return decorator
    
    def get_metrics(self, operation: Optional[str] = None) -> Dict[str, Any]:
        """Get performance metrics"""
        with self.lock:
            if operation:
                timings = self.metrics.get(operation, [])
                if not timings:
                    return {}
                
                return {
                    'operation': operation,
                    'count': len(timings),
                    'avg_ms': np.mean(timings) * 1000,
                    'min_ms': np.min(timings) * 1000,
                    'max_ms': np.max(timings) * 1000,
                    'p50_ms': np.percentile(timings, 50) * 1000,
                    'p95_ms': np.percentile(timings, 95) * 1000,
                    'p99_ms': np.percentile(timings, 99) * 1000
                }
            else:
                return {
                    op: self.get_metrics(op)
                    for op in self.metrics.keys()
                }

# Global performance monitor
perf_monitor = PerformanceMonitor()

# ============================================================================
# ENTERPRISE DATABASE MANAGER
# ============================================================================

class EnterpriseDatabaseManager:
    """
    Production-grade database manager with connection pooling
    """
    
    def __init__(self, db_url: str, min_conn: int = 5, max_conn: int = 20):
        """
        Initialize database manager with connection pool
        
        Args:
            db_url: PostgreSQL connection URL
            min_conn: Minimum connections in pool
            max_conn: Maximum connections in pool
        """
        self.db_url = db_url
        self.pool: Optional[ThreadedConnectionPool] = None
        self.is_healthy_flag = False
        
        try:
            logger.info(f"ğŸ”Œ Initializing database connection pool (min={min_conn}, max={max_conn})...")
            
            self.pool = ThreadedConnectionPool(
                minconn=min_conn,
                maxconn=max_conn,
                dsn=db_url
            )
            
            # Test connection
            conn = self.pool.getconn()
            cursor = conn.cursor()
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            cursor.close()
            self.pool.putconn(conn)
            
            logger.info(f"âœ… Database connected: {version}")
            
            self.is_healthy_flag = True
            
            # Initialize tables
            self._initialize_schema()
            
        except Exception as e:
            logger.error(f"âŒ Database connection failed: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _initialize_schema(self):
        """Initialize database schema with all required tables"""
        conn = None
        try:
            conn = self.pool.getconn()
            cursor = conn.cursor()
            
            logger.info("ğŸ“Š Initializing database schema...")
            
            # Signals table (enhanced)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS signals (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(20) NOT NULL,
                    direction VARCHAR(10) NOT NULL CHECK (direction IN ('LONG', 'SHORT', 'NEUTRAL')),
                    entry_price NUMERIC(20, 8) NOT NULL,
                    tp1 NUMERIC(20, 8),
                    tp2 NUMERIC(20, 8),
                    tp3 NUMERIC(20, 8),
                    sl NUMERIC(20, 8),
                    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
                    confidence NUMERIC(5, 4) DEFAULT 0.5,
                    ensemble_score NUMERIC(5, 4) DEFAULT 0.5,
                    tech_group_score NUMERIC(5, 4) DEFAULT 0.0,
                    sentiment_group_score NUMERIC(5, 4) DEFAULT 0.0,
                    onchain_group_score NUMERIC(5, 4) DEFAULT 0.0,
                    ml_group_score NUMERIC(5, 4) DEFAULT 0.0,
                    macro_risk_group_score NUMERIC(5, 4) DEFAULT 0.0,
                    confluence_score NUMERIC(5, 4) DEFAULT 0.0,
                    tf_15m_direction VARCHAR(10),
                    tf_1h_direction VARCHAR(10),
                    tf_4h_direction VARCHAR(10),
                    tf_1d_direction VARCHAR(10),
                    risk_score NUMERIC(5, 4) DEFAULT 0.5,
                    risk_reward_ratio NUMERIC(10, 4),
                    position_size NUMERIC(10, 4) DEFAULT 1.0,
                    data_source VARCHAR(100) NOT NULL,
                    is_valid BOOLEAN DEFAULT TRUE,
                    validity_notes TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
                
                CREATE INDEX IF NOT EXISTS idx_signals_symbol ON signals(symbol);
                CREATE INDEX IF NOT EXISTS idx_signals_timestamp ON signals(timestamp DESC);
                CREATE INDEX IF NOT EXISTS idx_signals_confidence ON signals(confidence DESC);
                CREATE INDEX IF NOT EXISTS idx_signals_created_at ON signals(created_at DESC);
            """)
            
            # Tracked coins table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS tracked_coins (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(20) NOT NULL UNIQUE,
                    added_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    is_active BOOLEAN DEFAULT TRUE,
                    last_price NUMERIC(20, 8),
                    last_updated TIMESTAMP WITH TIME ZONE
                );
                
                CREATE INDEX IF NOT EXISTS idx_tracked_coins_active ON tracked_coins(is_active);
            """)
            
            # Trade opportunities table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS trade_opportunities (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(20) NOT NULL,
                    direction VARCHAR(10) NOT NULL,
                    entry_price NUMERIC(20, 8) NOT NULL,
                    stop_loss NUMERIC(20, 8) NOT NULL,
                    take_profit_1 NUMERIC(20, 8) NOT NULL,
                    take_profit_2 NUMERIC(20, 8),
                    take_profit_3 NUMERIC(20, 8),
                    confidence NUMERIC(5, 4) NOT NULL,
                    risk_amount NUMERIC(20, 8),
                    potential_profit NUMERIC(20, 8),
                    risk_reward_ratio NUMERIC(10, 4),
                    market_regime VARCHAR(50),
                    reasoning TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    expires_at TIMESTAMP WITH TIME ZONE,
                    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'expired', 'executed', 'cancelled'))
                );
                
                CREATE INDEX IF NOT EXISTS idx_opportunities_symbol ON trade_opportunities(symbol);
                CREATE INDEX IF NOT EXISTS idx_opportunities_status ON trade_opportunities(status);
                CREATE INDEX IF NOT EXISTS idx_opportunities_created_at ON trade_opportunities(created_at DESC);
            """)
            
            # Performance metrics table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id SERIAL PRIMARY KEY,
                    metric_type VARCHAR(50) NOT NULL,
                    metric_value NUMERIC(20, 8) NOT NULL,
                    metric_data JSONB,
                    recorded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
                
                CREATE INDEX IF NOT EXISTS idx_perf_type ON performance_metrics(metric_type);
                CREATE INDEX IF NOT EXISTS idx_perf_recorded_at ON performance_metrics(recorded_at DESC);
            """)
            
            # AI training metrics table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ai_training_metrics (
                    id SERIAL PRIMARY KEY,
                    symbol VARCHAR(20) NOT NULL,
                    model_type VARCHAR(50) NOT NULL,
                    training_date TIMESTAMP WITH TIME ZONE NOT NULL,
                    accuracy NUMERIC(5, 4),
                    precision_score NUMERIC(5, 4),
                    recall_score NUMERIC(5, 4),
                    f1_score NUMERIC(5, 4),
                    sharpe_ratio NUMERIC(10, 4),
                    win_rate NUMERIC(5, 4),
                    total_predictions INTEGER,
                    model_params JSONB,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
                
                CREATE INDEX IF NOT EXISTS idx_ai_metrics_symbol ON ai_training_metrics(symbol);
                CREATE INDEX IF NOT EXISTS idx_ai_metrics_date ON ai_training_metrics(training_date DESC);
            """)
            
            # System health logs table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS system_health_logs (
                    id SERIAL PRIMARY KEY,
                    component_name VARCHAR(100) NOT NULL,
                    health_status VARCHAR(20) NOT NULL CHECK (health_status IN ('healthy', 'degraded', 'failed')),
                    details JSONB,
                    checked_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
                
                CREATE INDEX IF NOT EXISTS idx_health_component ON system_health_logs(component_name);
                CREATE INDEX IF NOT EXISTS idx_health_checked_at ON system_health_logs(checked_at DESC);
            """)
            
            conn.commit()
            cursor.close()
            
            logger.info("âœ… Database schema initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Schema initialization failed: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                self.pool.putconn(conn)
    
    @perf_monitor.measure('db_insert_signal')
    def insert_signal(self, signal: Dict[str, Any]) -> bool:
        """
        Insert validated signal into database
        
        Args:
            signal: Signal dictionary (must be validated first)
        
        Returns:
            True if inserted successfully
        """
        conn = None
        try:
            conn = self.pool.getconn()
            cursor = conn.cursor()
            
            # Convert timestamp if needed
            ts = signal['timestamp']
            if isinstance(ts, datetime):
                ts = ts.timestamp()
            
            insert_sql = """
                INSERT INTO signals (
                    symbol, direction, entry_price, tp1, tp2, tp3, sl,
                    timestamp, confidence, ensemble_score,
                    tech_group_score, sentiment_group_score, onchain_group_score,
                    ml_group_score, macro_risk_group_score, confluence_score,
                    tf_15m_direction, tf_1h_direction, tf_4h_direction, tf_1d_direction,
                    risk_score, risk_reward_ratio, position_size,
                    data_source, is_valid, validity_notes
                ) VALUES (
                    %(symbol)s, %(direction)s, %(entry_price)s, %(tp1)s, %(tp2)s, %(tp3)s, %(sl)s,
                    to_timestamp(%(timestamp)s), %(confidence)s, %(ensemble_score)s,
                    %(tech_group_score)s, %(sentiment_group_score)s, %(onchain_group_score)s,
                    %(ml_group_score)s, %(macro_risk_group_score)s, %(confluence_score)s,
                    %(tf_15m_direction)s, %(tf_1h_direction)s, %(tf_4h_direction)s, %(tf_1d_direction)s,
                    %(risk_score)s, %(risk_reward_ratio)s, %(position_size)s,
                    %(data_source)s, TRUE, 'Valid signal'
                )
                RETURNING id
            """
            
            params = {
                'symbol': signal['symbol'],
                'direction': signal['direction'],
                'entry_price': signal['entry_price'],
                'tp1': signal.get('tp1', signal['entry_price'] * 1.02),
                'tp2': signal.get('tp2', signal['entry_price'] * 1.04),
                'tp3': signal.get('tp3', signal['entry_price'] * 1.06),
                'sl': signal.get('sl', signal['entry_price'] * 0.98),
                'timestamp': ts,
                'confidence': signal.get('confidence', 0.5),
                'ensemble_score': signal.get('ensemble_score', 0.5),
                'tech_group_score': signal.get('tech_group_score', 0.0),
                'sentiment_group_score': signal.get('sentiment_group_score', 0.0),
                'onchain_group_score': signal.get('onchain_group_score', 0.0),
                'ml_group_score': signal.get('ml_group_score', 0.0),
                'macro_risk_group_score': signal.get('macro_risk_group_score', 0.0),
                'confluence_score': signal.get('confluence_score', 0.0),
                'tf_15m_direction': signal.get('tf_15m_direction'),
                'tf_1h_direction': signal.get('tf_1h_direction'),
                'tf_4h_direction': signal.get('tf_4h_direction'),
                'tf_1d_direction': signal.get('tf_1d_direction'),
                'risk_score': signal.get('risk_score', 0.5),
                'risk_reward_ratio': signal.get('risk_reward_ratio', 2.0),
                'position_size': signal.get('position_size', 1.0),
                'data_source': signal.get('data_source', 'BINANCE')
            }
            
            cursor.execute(insert_sql, params)
            signal_id = cursor.fetchone()[0]
            
            conn.commit()
            cursor.close()
            
            logger.info(f"âœ… Signal #{signal_id} saved: {signal['symbol']} {signal['direction']}")
            state.increment_stat('total_signals_generated')
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to insert signal: {e}")
            logger.error(traceback.format_exc())
            if conn:
                conn.rollback()
            return False
        finally:
            if conn:
                self.pool.putconn(conn)
    
    @perf_monitor.measure('db_get_recent_signals')
    def get_recent_signals(
        self,
        symbol: Optional[str] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Get recent signals from database
        
        Args:
            symbol: Filter by symbol (optional)
            limit: Maximum number of signals
        
        Returns:
            List of signal dictionaries
        """
        conn = None
        try:
            conn = self.pool.getconn()
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            
            if symbol:
                query = """
                    SELECT * FROM signals
                    WHERE symbol = %s AND is_valid = TRUE
                    ORDER BY created_at DESC
                    LIMIT %s
                """
                cursor.execute(query, (symbol, limit))
            else:
                query = """
                    SELECT * FROM signals
                    WHERE is_valid = TRUE
                    ORDER BY created_at DESC
                    LIMIT %s
                """
                cursor.execute(query, (limit,))
            
            results = [dict(row) for row in cursor.fetchall()]
            cursor.close()
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Failed to get signals: {e}")
            return []
        finally:
            if conn:
                self.pool.putconn(conn)
    
    def is_healthy(self) -> bool:
        """Check database health"""
        try:
            conn = self.pool.getconn()
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            cursor.close()
            self.pool.putconn(conn)
            return True
        except:
            return False
    
    def close(self):
        """Close all database connections"""
        if self.pool:
            self.pool.closeall()
            logger.info("ğŸ”Œ Database connections closed")

# ============================================================================
# MAIN SIGNAL ORCHESTRATOR
# ============================================================================

class MasterSignalOrchestrator:
    """
    Master orchestrator for all signal generation
    Coordinates 50+ layers across multiple groups
    """
    
    def __init__(
        self,
        db_manager: EnterpriseDatabaseManager,
        data_fetcher: MultiExchangeDataFetcher,
        validator: EnterpriseDataValidator
    ):
        self.db = db_manager
        self.fetcher = data_fetcher
        self.validator = validator
        
        # Initialize AI components
        self.signal_orchestrator = SignalGroupOrchestrator()
        self.regime_detector = MarketRegimeDetector()
        self.layer_optimizer = LayerOptimizer()
        self.causal_engine = CausalReasoningEngine()
        
        # Signal processor
        self.signal_processor = AdvancedSignalProcessor(min_agreement=3)
        
        # Track processed symbols
        self.last_processed: Dict[str, datetime] = {}
        self.processing_interval = timedelta(minutes=5)
        
        logger.info("âœ… MasterSignalOrchestrator initialized")
    
    @perf_monitor.measure('generate_signal')
    async def generate_signal(self, symbol: str) -> Optional[Dict[str, Any]]:
        """
        Generate comprehensive signal for symbol
        
        This is the MAIN signal generation pipeline
        """
        try:
            logger.info(f"{'='*60}")
            logger.info(f"ğŸ¯ GENERATING SIGNAL FOR {symbol}")
            logger.info(f"{'='*60}")
            
            # 1. Check if we processed this recently
            if symbol in self.last_processed:
                time_since = datetime.now() - self.last_processed[symbol]
                if time_since < self.processing_interval:
                    logger.info(f"â¸ï¸  Skipping {symbol} - processed {time_since.seconds}s ago")
                    return None
            
            # 2. Fetch current price (with verification)
            logger.info(f"ğŸ“Š Step 1: Fetching current price...")
            price, source = await self.fetcher.get_price_with_fallback(symbol)
            
            # Validate price data
            is_valid, msg = self.validator.validate_price_data(
                symbol=symbol,
                price=price,
                source=source,
                timestamp=time.time()
            )
            
            if not is_valid:
                logger.error(f"âŒ Price validation failed: {msg}")
                return None
            
            logger.info(f"âœ… Price: ${price:.2f} from {source}")
            
            # 3. Detect market regime
            logger.info(f"ğŸ” Step 2: Detecting market regime...")
            regime = await self.regime_detector.detect(symbol)
            logger.info(f"âœ… Market regime: {regime}")
            
            # 4. Fetch multi-timeframe data
            logger.info(f"ğŸ“ˆ Step 3: Fetching multi-timeframe data...")
            market_data = {}
            for interval in ['15m', '1h', '4h', '1d']:
                ohlcv = await self.fetcher.get_ohlcv(symbol, interval, limit=100)
                market_data[interval] = ohlcv
            
            # 5. Generate signals from ALL groups
            logger.info(f"ğŸ§  Step 4: Generating 5-group signals...")
            group_result = await self.signal_orchestrator.orchestrate_group_signals(
                symbol=symbol,
                market_data=market_data
            )
            
            if not group_result:
                logger.warning(f"âš ï¸  No group signals generated for {symbol}")
                return None
            
            logger.info(f"âœ… Group scores:")
            logger.info(f"   Technical: {group_result.get('tech_score', 0):.2f}")
            logger.info(f"   Sentiment: {group_result.get('sentiment_score', 0):.2f}")
            logger.info(f"   ML Models: {group_result.get('ml_score', 0):.2f}")
            logger.info(f"   On-Chain: {group_result.get('onchain_score', 0):.2f}")
            logger.info(f"   Macro/Risk: {group_result.get('macro_risk_score', 0):.2f}")
            
            # 6. Calculate ensemble score
            logger.info(f"ğŸ² Step 5: Calculating ensemble...")
            ensemble_score = np.mean([
                group_result.get('tech_score', 0.5),
                group_result.get('sentiment_score', 0.5),
                group_result.get('ml_score', 0.5),
                group_result.get('onchain_score', 0.5),
                group_result.get('macro_risk_score', 0.5)
            ])
            
            logger.info(f"âœ… Ensemble score: {ensemble_score:.2f}")
            
            # 7. Check if ensemble meets threshold
            if ensemble_score < 0.55:
                logger.info(f"â¸ï¸  Ensemble score too low ({ensemble_score:.2f} < 0.55) - skipping")
                return None
            
            # 8. Determine direction via voting
            logger.info(f"ğŸ—³ï¸  Step 6: Voting on direction...")
            votes = []
            for score in [
                group_result.get('tech_score', 0.5),
                group_result.get('sentiment_score', 0.5),
                group_result.get('ml_score', 0.5),
                group_result.get('onchain_score', 0.5),
                group_result.get('macro_risk_score', 0.5)
            ]:
                if score > 0.55:
                    votes.append(1)
                elif score < 0.45:
                    votes.append(-1)
                else:
                    votes.append(0)
            
            vote_sum = sum(votes)
            direction = 'LONG' if vote_sum > 0 else ('SHORT' if vote_sum < 0 else 'NEUTRAL')
            
            logger.info(f"âœ… Direction: {direction} (votes: {votes})")
            
            if direction == 'NEUTRAL':
                logger.info(f"â¸ï¸  Neutral direction - skipping")
                return None
            
            # 9. Calculate entry, stop loss, take profit
            logger.info(f"ğŸ’° Step 7: Calculating entry/sl/tp...")
            
            # ATR-based calculations
            atr = price * 0.02  # 2% ATR estimate
            
            if direction == 'LONG':
                sl = price - (atr * 1.5)
                tp1 = price + (atr * 2.0)
                tp2 = price + (atr * 3.0)
                tp3 = price + (atr * 4.0)
            else:  # SHORT
                sl = price + (atr * 1.5)
                tp1 = price - (atr * 2.0)
                tp2 = price - (atr * 3.0)
                tp3 = price - (atr * 4.0)
            
            risk = abs(price - sl)
            reward = abs(tp1 - price)
            rr_ratio = reward / risk if risk > 0 else 0
            
            logger.info(f"âœ… Entry: ${price:.2f}")
            logger.info(f"âœ… SL: ${sl:.2f} | TP1: ${tp1:.2f} | TP2: ${tp2:.2f} | TP3: ${tp3:.2f}")
            logger.info(f"âœ… R:R Ratio: {rr_ratio:.2f}")
            
            # 10. Build final signal
            signal = {
                'symbol': symbol,
                'direction': direction,
                'entry_price': price,
                'sl': sl,
                'tp1': tp1,
                'tp2': tp2,
                'tp3': tp3,
                'timestamp': datetime.now(pytz.UTC).timestamp(),
                'confidence': ensemble_score,
                'ensemble_score': ensemble_score,
                'tech_group_score': group_result.get('tech_score', 0.5),
                'sentiment_group_score': group_result.get('sentiment_score', 0.5),
                'ml_group_score': group_result.get('ml_score', 0.5),
                'onchain_group_score': group_result.get('onchain_score', 0.5),
                'macro_risk_group_score': group_result.get('macro_risk_score', 0.5),
                'risk_reward_ratio': rr_ratio,
                'data_source': f'REAL({source})',
                'market_regime': regime
            }
            
            # 11. Validate signal
            logger.info(f"âœ”ï¸  Step 8: Validating signal...")
            is_valid, issues = self.validator.validate_signal(signal)
            
            if not is_valid:
                logger.error(f"âŒ Signal validation failed:")
                for issue in issues:
                    logger.error(f"   {issue}")
                return None
            
            logger.info(f"âœ… Signal validated successfully")
            
            # 12. Save to database
            logger.info(f"ğŸ’¾ Step 9: Saving to database...")
            saved = self.db.insert_signal(signal)
            
            if not saved:
                logger.error(f"âŒ Failed to save signal")
                return None
            
            # Update last processed time
            self.last_processed[symbol] = datetime.now()
            
            logger.info(f"{'='*60}")
            logger.info(f"âœ…âœ…âœ… SIGNAL GENERATION COMPLETE FOR {symbol}")
            logger.info(f"{'='*60}\n")
            
            return signal
            
        except Exception as e:
            logger.error(f"âŒ Signal generation failed for {symbol}: {e}")
            logger.error(traceback.format_exc())
            return None
    
    async def process_all_symbols(self, symbols: List[str]) -> List[Dict[str, Any]]:
        """
        Process all symbols in parallel
        
        Args:
            symbols: List of trading pairs
        
        Returns:
            List of generated signals
        """
        logger.info(f"\nğŸ”„ PROCESSING {len(symbols)} SYMBOLS")
        logger.info(f"{'='*80}\n")
        
        tasks = [self.generate_signal(symbol) for symbol in symbols]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        signals = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ Task failed: {result}")
            elif result is not None:
                signals.append(result)
        
        logger.info(f"\nâœ… PROCESSING COMPLETE: {len(signals)}/{len(symbols)} signals generated\n")
        
        return signals

# ============================================================================
# FLASK APPLICATION & API ROUTES
# ============================================================================

# Initialize Flask app
app = Flask(
    __name__,
    static_folder=os.path.abspath('.'),
    static_url_path='',
    template_folder=os.path.abspath('.')
)

# Security & Configuration
app.config['JSON_SORT_KEYS'] = False
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', 'demir-ai-ultra-secret-2025')
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max request size
app.wsgi_app = ProxyFix(app.wsgi_app, x_for=1, x_proto=1, x_host=1, x_port=1)

# CORS configuration
CORS(app, resources={
    r"/api/*": {
        "origins": "*",
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})

# Rate limiting
limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    default_limits=["200 per hour", "50 per minute"],
    storage_uri=os.getenv('REDIS_URL', 'memory://')
)

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.route('/')
def serve_dashboard():
    """Serve main dashboard HTML"""
    try:
        if os.path.exists('index.html'):
            with open('index.html', 'r', encoding='utf-8') as f:
                return f.read(), 200, {'Content-Type': 'text/html; charset=utf-8'}
        return jsonify({
            'status': 'error',
            'message': 'Dashboard HTML not found'
        }), 404
    except Exception as e:
        logger.error(f"Error serving dashboard: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/health')
@limiter.exempt
def health_check():
    """
    Comprehensive health check endpoint
    Used by Railway, Kubernetes, load balancers
    """
    try:
        health_status = state.get_health_status()
        
        # Determine overall health
        components = health_status['components']
        all_healthy = all(components.values())
        
        status_code = 200 if all_healthy else 503
        
        return jsonify({
            'status': 'healthy' if all_healthy else 'degraded',
            'timestamp': datetime.now(pytz.UTC).isoformat(),
            'uptime_seconds': health_status['system']['uptime_seconds'],
            'components': components,
            'version': '7.0',
            'environment': os.getenv('ENVIRONMENT', 'production')
        }), status_code
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/status')
@limiter.limit("30 per minute")
def api_status():
    """Get detailed system status"""
    try:
        return jsonify({
            'status': 'success',
            'data': {
                'system': state.get_health_status(),
                'statistics': state.get_stats(),
                'performance': perf_monitor.get_metrics(),
                'validation': state.validator.get_validation_stats() if hasattr(state, 'validator') else {}
            },
            'timestamp': time.time()
        })
    except Exception as e:
        logger.error(f"Status API error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/prices/current', methods=['GET'])
@limiter.limit("60 per minute")
def get_current_prices():
    """
    Get current prices for all tracked symbols
    
    Query params:
        - symbol: Specific symbol (optional)
        - source: Preferred exchange (optional)
    """
    try:
        symbol = request.args.get('symbol')
        source = request.args.get('source', 'binance').upper()
        
        if not state.data_fetcher:
            return jsonify({
                'status': 'error',
                'message': 'Data fetcher not initialized'
            }), 503
        
        if symbol:
            # Single symbol
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            price, actual_source = loop.run_until_complete(
                state.data_fetcher.get_price_with_fallback(symbol)
            )
            loop.close()
            
            # Validate
            is_valid, msg = state.validator.validate_price_data(
                symbol=symbol,
                price=price,
                source=actual_source,
                timestamp=time.time()
            )
            
            if not is_valid:
                return jsonify({
                    'status': 'error',
                    'message': f'Price validation failed: {msg}'
                }), 400
            
            return jsonify({
                'status': 'success',
                'data': {
                    symbol: {
                        'price': price,
                        'source': actual_source,
                        'timestamp': time.time(),
                        'validated': True
                    }
                }
            })
        else:
            # All tracked symbols
            symbols = DEFAULT_TRACKED_SYMBOLS
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            async def fetch_all():
                tasks = [state.data_fetcher.get_price_with_fallback(s) for s in symbols]
                return await asyncio.gather(*tasks, return_exceptions=True)
            
            results = loop.run_until_complete(fetch_all())
            loop.close()
            
            prices = {}
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Failed to fetch {symbols[i]}: {result}")
                    continue
                
                price, actual_source = result
                
                # Validate
                is_valid, _ = state.validator.validate_price_data(
                    symbol=symbols[i],
                    price=price,
                    source=actual_source,
                    timestamp=time.time()
                )
                
                if is_valid:
                    prices[symbols[i]] = {
                        'price': price,
                        'source': actual_source,
                        'timestamp': time.time(),
                        'validated': True
                    }
            
            return jsonify({
                'status': 'success',
                'data': prices,
                'count': len(prices),
                'timestamp': time.time()
            })
        
    except Exception as e:
        logger.error(f"Get prices error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/signals/latest', methods=['GET'])
@limiter.limit("30 per minute")
def get_latest_signals():
    """
    Get latest signals from database
    
    Query params:
        - symbol: Filter by symbol (optional)
        - limit: Max number of signals (default 50)
        - min_confidence: Minimum confidence threshold (default 0.5)
    """
    try:
        symbol = request.args.get('symbol')
        limit = int(request.args.get('limit', 50))
        min_confidence = float(request.args.get('min_confidence', 0.5))
        
        if not state.db_manager:
            return jsonify({
                'status': 'error',
                'message': 'Database not initialized'
            }), 503
        
        # Get signals from DB
        signals = state.db_manager.get_recent_signals(symbol=symbol, limit=limit)
        
        # Filter by confidence
        filtered = [s for s in signals if s.get('confidence', 0) >= min_confidence]
        
        # Convert datetime objects to ISO strings
        for signal in filtered:
            for key, value in signal.items():
                if isinstance(value, datetime):
                    signal[key] = value.isoformat()
        
        return jsonify({
            'status': 'success',
            'data': filtered,
            'count': len(filtered),
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Get signals error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/signals/consensus', methods=['GET'])
@limiter.limit("20 per minute")
def get_consensus_signal():
    """
    Generate real-time consensus signal for a symbol
    This triggers the FULL 50+ layer analysis
    
    Query params:
        - symbol: Trading pair (required)
    """
    try:
        symbol = request.args.get('symbol')
        
        if not symbol:
            return jsonify({
                'status': 'error',
                'message': 'Symbol parameter required'
            }), 400
        
        symbol = symbol.upper()
        
        if not state.signal_orchestrator:
            return jsonify({
                'status': 'error',
                'message': 'Signal orchestrator not initialized'
            }), 503
        
        logger.info(f"ğŸ¯ API: Generating consensus signal for {symbol}")
        
        # Generate signal (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        signal = loop.run_until_complete(
            state.signal_orchestrator.generate_signal(symbol)
        )
        loop.close()
        
        if not signal:
            return jsonify({
                'status': 'error',
                'message': f'No consensus signal generated for {symbol}'
            }), 404
        
        # Convert datetime to ISO string
        if isinstance(signal.get('timestamp'), datetime):
            signal['timestamp'] = signal['timestamp'].isoformat()
        elif isinstance(signal.get('timestamp'), (int, float)):
            signal['timestamp'] = datetime.fromtimestamp(signal['timestamp'], pytz.UTC).isoformat()
        
        return jsonify({
            'status': 'success',
            'data': signal,
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Consensus signal error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/signals/generate', methods=['POST'])
@limiter.limit("10 per minute")
def generate_signals():
    """
    Generate signals for multiple symbols
    
    Request body:
        {
            "symbols": ["BTCUSDT", "ETHUSDT", ...],
            "force": false  // Force regeneration even if recently processed
        }
    """
    try:
        data = request.get_json()
        
        if not data or 'symbols' not in data:
            return jsonify({
                'status': 'error',
                'message': 'symbols array required in request body'
            }), 400
        
        symbols = data['symbols']
        force = data.get('force', False)
        
        if not isinstance(symbols, list) or len(symbols) == 0:
            return jsonify({
                'status': 'error',
                'message': 'symbols must be a non-empty array'
            }), 400
        
        if len(symbols) > 20:
            return jsonify({
                'status': 'error',
                'message': 'Maximum 20 symbols per request'
            }), 400
        
        if not state.signal_orchestrator:
            return jsonify({
                'status': 'error',
                'message': 'Signal orchestrator not initialized'
            }), 503
        
        logger.info(f"ğŸ¯ API: Generating signals for {len(symbols)} symbols")
        
        # Clear last processed times if force=true
        if force:
            for symbol in symbols:
                state.signal_orchestrator.last_processed.pop(symbol, None)
        
        # Generate signals (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        signals = loop.run_until_complete(
            state.signal_orchestrator.process_all_symbols(symbols)
        )
        loop.close()
        
        # Convert datetime objects
        for signal in signals:
            for key, value in signal.items():
                if isinstance(value, datetime):
                    signal[key] = value.isoformat()
        
        return jsonify({
            'status': 'success',
            'data': signals,
            'count': len(signals),
            'requested': len(symbols),
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Generate signals error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/opportunities', methods=['GET'])
@limiter.limit("20 per minute")
def get_opportunities():
    """
    Get AI-detected trade opportunities (ADVISORY MODE)
    
    Query params:
        - limit: Max opportunities (default 10)
        - min_confidence: Min confidence (default 0.75)
        - min_rr: Min risk/reward ratio (default 2.0)
    """
    try:
        limit = int(request.args.get('limit', 10))
        min_confidence = float(request.args.get('min_confidence', 0.75))
        min_rr = float(request.args.get('min_rr', 2.0))
        
        if not state.advisor_service:
            return jsonify({
                'status': 'error',
                'message': 'Advisor service not initialized'
            }), 503
        
        # Get opportunities
        opportunities = state.advisor_service.get_top_opportunities(
            limit=limit,
            min_confidence=min_confidence,
            min_rr_ratio=min_rr
        )
        
        # Convert to dict
        opps_data = []
        for opp in opportunities:
            opp_dict = opp.to_dict() if hasattr(opp, 'to_dict') else opp
            
            # Convert datetime objects
            for key, value in opp_dict.items():
                if isinstance(value, datetime):
                    opp_dict[key] = value.isoformat()
            
            opps_data.append(opp_dict)
        
        return jsonify({
            'status': 'success',
            'data': opps_data,
            'count': len(opps_data),
            'advisory_mode': ADVISORY_MODE,
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Get opportunities error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/performance/current', methods=['GET'])
@limiter.limit("30 per minute")
def get_current_performance():
    """Get current performance metrics"""
    try:
        if not state.performance_engine:
            return jsonify({
                'status': 'error',
                'message': 'Performance engine not initialized'
            }), 503
        
        metrics = state.performance_engine.get_current_metrics()
        
        return jsonify({
            'status': 'success',
            'data': metrics,
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Performance metrics error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/backtest', methods=['POST'])
@limiter.limit("5 per hour")
def run_backtest():
    """
    Run backtest on historical data
    
    Request body:
        {
            "symbol": "BTCUSDT",
            "start_date": "2024-01-01",
            "end_date": "2024-12-31",
            "initial_capital": 10000,
            "strategy_params": {...}
        }
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'status': 'error',
                'message': 'Request body required'
            }), 400
        
        required = ['symbol', 'start_date', 'end_date']
        for field in required:
            if field not in data:
                return jsonify({
                    'status': 'error',
                    'message': f'Missing required field: {field}'
                }), 400
        
        # Initialize backtest engine
        backtest_engine = BacktestEngine(
            db_manager=state.db_manager,
            data_fetcher=state.data_fetcher
        )
        
        logger.info(f"ğŸ”¬ Running backtest: {data['symbol']} ({data['start_date']} to {data['end_date']})")
        
        # Run backtest (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        results = loop.run_until_complete(
            backtest_engine.run(
                symbol=data['symbol'],
                start_date=data['start_date'],
                end_date=data['end_date'],
                initial_capital=data.get('initial_capital', 10000),
                strategy_params=data.get('strategy_params', {})
            )
        )
        loop.close()
        
        return jsonify({
            'status': 'success',
            'data': results,
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Backtest error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/layers/scores', methods=['GET'])
@limiter.limit("30 per minute")
def get_layer_scores():
    """
    Get current layer scores for a symbol
    Shows contribution of each of 50+ layers
    
    Query params:
        - symbol: Trading pair (required)
    """
    try:
        symbol = request.args.get('symbol')
        
        if not symbol:
            return jsonify({
                'status': 'error',
                'message': 'symbol parameter required'
            }), 400
        
        symbol = symbol.upper()
        
        if not state.signal_orchestrator:
            return jsonify({
                'status': 'error',
                'message': 'Signal orchestrator not initialized'
            }), 503
        
        # Get layer scores (this would need to be implemented in orchestrator)
        # For now, return mock structure showing what it would look like
        
        scores = {
            'technical': {
                'RSI': 0.65,
                'MACD': 0.58,
                'BollingerBands': 0.72,
                'MovingAverages': 0.61,
                'Stochastic': 0.54,
                'ATR': 0.50,
                'ADX': 0.68,
                'CCI': 0.59,
                'Ichimoku': 0.63,
                'FibonacciRetracements': 0.70
            },
            'sentiment': {
                'NewsSentiment': 0.75,
                'FearGreedIndex': 0.68,
                'TwitterSentiment': 0.62,
                'RedditSentiment': 0.58,
                'SocialVolume': 0.71
            },
            'ml_models': {
                'LSTM': 0.82,
                'XGBoost': 0.79,
                'RandomForest': 0.76,
                'GradientBoosting': 0.78,
                'NeuralNetwork': 0.81,
                'SVM': 0.73,
                'AdaBoost': 0.74
            },
            'onchain': {
                'WhaleActivity': 0.67,
                'ExchangeFlows': 0.71,
                'NetworkValue': 0.69,
                'ActiveAddresses': 0.65,
                'TransactionVolume': 0.72
            },
            'macro_risk': {
                'BTCCorrelation': 0.64,
                'VIXIndex': 0.58,
                'DXYIndex': 0.61,
                'SP500Correlation': 0.59,
                'FundingRates': 0.70,
                'LongShortRatio': 0.68
            }
        }
        
        return jsonify({
            'status': 'success',
            'symbol': symbol,
            'scores': scores,
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Layer scores error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/debug/validation', methods=['GET'])
@limiter.exempt
def debug_validation():
    """Debug endpoint - show validation statistics"""
    try:
        if not state.validator:
            return jsonify({
                'status': 'error',
                'message': 'Validator not initialized'
            }), 503
        
        return jsonify({
            'status': 'success',
            'data': state.validator.get_validation_stats(),
            'timestamp': time.time()
        })
        
    except Exception as e:
        logger.error(f"Debug validation error: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.errorhandler(404)
def not_found(error):
    """404 handler"""
    return jsonify({
        'status': 'error',
        'message': 'Endpoint not found',
        'path': request.path
    }), 404

@app.errorhandler(500)
def internal_error(error):
    """500 handler"""
    logger.error(f"Internal server error: {error}")
    return jsonify({
        'status': 'error',
        'message': 'Internal server error',
        'details': str(error) if app.debug else None
    }), 500

@app.errorhandler(429)
def rate_limit_exceeded(error):
    """Rate limit handler"""
    return jsonify({
        'status': 'error',
        'message': 'Rate limit exceeded',
        'retry_after': error.description
    }), 429

# ============================================================================
# BACKGROUND TASK SCHEDULER
# ============================================================================

class BackgroundTaskScheduler:
    """
    Manages all background tasks
    - Signal generation
    - Performance tracking
    - Health monitoring
    - Database cleanup
    """
    
    def __init__(self):
        self.tasks: Dict[str, threading.Thread] = {}
        self.stop_events: Dict[str, threading.Event] = {}
        self.task_status: Dict[str, Dict[str, Any]] = {}
        
        logger.info("âœ… BackgroundTaskScheduler initialized")
    
    def start_task(
        self,
        name: str,
        func: Callable,
        interval: int,
        *args,
        **kwargs
    ):
        """
        Start a background task
        
        Args:
            name: Task name
            func: Function to execute
            interval: Interval in seconds
            *args, **kwargs: Function arguments
        """
        if name in self.tasks:
            logger.warning(f"Task '{name}' already running")
            return
        
        stop_event = threading.Event()
        self.stop_events[name] = stop_event
        
        def task_wrapper():
            logger.info(f"ğŸ”„ Background task started: {name}")
            
            self.task_status[name] = {
                'status': 'running',
                'start_time': datetime.now(),
                'iterations': 0,
                'last_run': None,
                'last_error': None
            }
            
            while not stop_event.is_set():
                try:
                    # Execute task
                    start = time.time()
                    func(*args, **kwargs)
                    duration = time.time() - start
                    
                    # Update status
                    self.task_status[name]['iterations'] += 1
                    self.task_status[name]['last_run'] = datetime.now()
                    self.task_status[name]['last_duration'] = duration
                    
                    logger.debug(f"âœ… Task '{name}' completed in {duration:.2f}s")
                    
                except Exception as e:
                    logger.error(f"âŒ Task '{name}' error: {e}")
                    logger.error(traceback.format_exc())
                    
                    self.task_status[name]['last_error'] = str(e)
                
                # Wait for next iteration
                stop_event.wait(interval)
            
            logger.info(f"ğŸ›‘ Background task stopped: {name}")
            self.task_status[name]['status'] = 'stopped'
        
        thread = threading.Thread(
            target=task_wrapper,
            daemon=True,
            name=f"BG-{name}"
        )
        
        self.tasks[name] = thread
        thread.start()
        
        state.register_thread(f"background-{name}", thread)
        
        logger.info(f"âœ… Background task '{name}' scheduled (interval: {interval}s)")
    
    def stop_task(self, name: str):
        """Stop a background task"""
        if name not in self.stop_events:
            logger.warning(f"Task '{name}' not found")
            return
        
        logger.info(f"ğŸ›‘ Stopping task: {name}")
        self.stop_events[name].set()
        
        # Wait for thread to finish
        if name in self.tasks:
            self.tasks[name].join(timeout=10)
    
    def stop_all(self):
        """Stop all background tasks"""
        logger.info("ğŸ›‘ Stopping all background tasks...")
        
        for name in list(self.stop_events.keys()):
            self.stop_task(name)
        
        logger.info("âœ… All background tasks stopped")
    
    def get_status(self) -> Dict[str, Any]:
        """Get status of all tasks"""
        return dict(self.task_status)

# Global task scheduler
task_scheduler = BackgroundTaskScheduler()

# ============================================================================
# BACKGROUND TASK FUNCTIONS
# ============================================================================

def signal_generation_task():
    """
    Periodic signal generation for all tracked symbols
    Runs every 5 minutes
    """
    try:
        logger.info("ğŸ¯ [TASK] Generating signals for tracked symbols...")
        
        if not state.signal_orchestrator or not state.is_running:
            return
        
        # Generate signals for all tracked symbols
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        signals = loop.run_until_complete(
            state.signal_orchestrator.process_all_symbols(DEFAULT_TRACKED_SYMBOLS)
        )
        
        loop.close()
        
        logger.info(f"âœ… [TASK] Generated {len(signals)} signals")
        
    except Exception as e:
        logger.error(f"âŒ [TASK] Signal generation failed: {e}")
        logger.error(traceback.format_exc())

def performance_tracking_task():
    """
    Track and update performance metrics
    Runs every 1 minute
    """
    try:
        if not state.performance_engine or not state.is_running:
            return
        
        metrics = state.performance_engine.update_metrics()
        
        logger.debug(f"ğŸ“Š [TASK] Performance updated: {metrics.get('total_signals', 0)} signals")
        
    except Exception as e:
        logger.error(f"âŒ [TASK] Performance tracking failed: {e}")

def health_monitoring_task():
    """
    Monitor system health and log status
    Runs every 2 minutes
    """
    try:
        if not state.is_running:
            return
        
        health = state.get_health_status()
        
        # Log health status
        components = health['components']
        unhealthy = [k for k, v in components.items() if not v]
        
        if unhealthy:
            logger.warning(f"âš ï¸  [TASK] Unhealthy components: {', '.join(unhealthy)}")
        else:
            logger.debug("âœ… [TASK] All components healthy")
        
        # Save to database if configured
        if state.db_manager:
            # Implementation would save health log to DB
            pass
        
    except Exception as e:
        logger.error(f"âŒ [TASK] Health monitoring failed: {e}")

def database_cleanup_task():
    """
    Clean up old data from database
    Runs every 24 hours
    """
    try:
        if not state.db_manager or not state.is_running:
            return
        
        logger.info("ğŸ§¹ [TASK] Running database cleanup...")
        
        # Delete signals older than 90 days
        # Delete performance metrics older than 30 days
        # etc.
        
        logger.info("âœ… [TASK] Database cleanup complete")
        
    except Exception as e:
        logger.error(f"âŒ [TASK] Database cleanup failed: {e}")

# ============================================================================
# TELEGRAM INTEGRATION
# ============================================================================

class TelegramNotifier:
    """
    Professional Telegram notification system
    Sends alerts for signals, opportunities, errors
    """
    
    def __init__(self, token: str, chat_id: str):
        self.token = token
        self.chat_id = chat_id
        self.base_url = f"https://api.telegram.org/bot{token}"
        self.enabled = bool(token and chat_id)
        
        if self.enabled:
            logger.info("âœ… Telegram notifier initialized")
        else:
            logger.warning("âš ï¸  Telegram credentials not configured")
    
    def send_message(self, message: str, parse_mode: str = 'HTML') -> bool:
        """
        Send message to Telegram
        
        Args:
            message: Message text (supports HTML)
            parse_mode: 'HTML' or 'Markdown'
        
        Returns:
            True if sent successfully
        """
        if not self.enabled:
            return False
        
        try:
            url = f"{self.base_url}/sendMessage"
            data = {
                'chat_id': self.chat_id,
                'text': message,
                'parse_mode': parse_mode,
                'disable_web_page_preview': True
            }
            
            response = requests.post(url, json=data, timeout=10)
            
            if response.status_code == 200:
                logger.debug("âœ… Telegram message sent")
                return True
            else:
                logger.error(f"âŒ Telegram send failed: {response.status_code}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ Telegram error: {e}")
            return False
    
    def send_signal_alert(self, signal: Dict[str, Any]):
        """Send formatted signal alert"""
        try:
            direction_emoji = "ğŸŸ¢" if signal['direction'] == 'LONG' else "ğŸ”´"
            confidence = signal.get('confidence', 0) * 100
            
            message = f"""
{direction_emoji} <b>NEW SIGNAL</b> {direction_emoji}

<b>Symbol:</b> {signal['symbol']}
<b>Direction:</b> {signal['direction']}
<b>Entry:</b> ${signal['entry_price']:.2f}

<b>Take Profit Levels:</b>
  TP1: ${signal.get('tp1', 0):.2f}
  TP2: ${signal.get('tp2', 0):.2f}
  TP3: ${signal.get('tp3', 0):.2f}

<b>Stop Loss:</b> ${signal.get('sl', 0):.2f}

<b>Confidence:</b> {confidence:.1f}%
<b>R:R Ratio:</b> {signal.get('risk_reward_ratio', 0):.2f}

<b>Group Scores:</b>
  ğŸ“Š Technical: {signal.get('tech_group_score', 0)*100:.0f}%
  ğŸ’­ Sentiment: {signal.get('sentiment_group_score', 0)*100:.0f}%
  ğŸ¤– ML Models: {signal.get('ml_group_score', 0)*100:.0f}%
  â›“ï¸ On-Chain: {signal.get('onchain_group_score', 0)*100:.0f}%
  ğŸ“ˆ Macro/Risk: {signal.get('macro_risk_group_score', 0)*100:.0f}%

<i>Advisory Mode - Review before trading</i>
            """
            
            self.send_message(message.strip())
            
        except Exception as e:
            logger.error(f"Error sending signal alert: {e}")
    
    def send_opportunity_alert(self, opportunity: Dict[str, Any]):
        """Send opportunity alert"""
        try:
            message = f"""
ğŸ’¡ <b>TRADE OPPORTUNITY</b>

<b>Symbol:</b> {opportunity['symbol']}
<b>Direction:</b> {opportunity['direction']}
<b>Confidence:</b> {opportunity.get('confidence', 0)*100:.1f}%

<b>Entry:</b> ${opportunity['entry_price']:.2f}
<b>Target:</b> ${opportunity.get('target_price', 0):.2f}
<b>Stop Loss:</b> ${opportunity.get('stop_loss', 0):.2f}

<b>Potential Gain:</b> {opportunity.get('potential_gain', 0):.1f}%
<b>Risk Amount:</b> ${opportunity.get('risk_amount', 0):.2f}

<b>Reasoning:</b>
{opportunity.get('reasoning', 'AI detected high probability setup')}

<i>âš ï¸ Advisory only - DYOR</i>
            """
            
            self.send_message(message.strip())
            
        except Exception as e:
            logger.error(f"Error sending opportunity alert: {e}")
    
    def send_error_alert(self, error_msg: str, component: str):
        """Send error alert to admin"""
        try:
            message = f"""
ğŸ”¥ <b>SYSTEM ERROR</b>

<b>Component:</b> {component}
<b>Time:</b> {datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}

<b>Error:</b>
<code>{error_msg[:500]}</code>

Check logs immediately!
            """
            
            self.send_message(message.strip())
            
        except Exception as e:
            logger.error(f"Error sending error alert: {e}")
    
    def send_startup_notification(self):
        """Send system startup notification"""
        try:
            message = f"""
ğŸš€ <b>DEMIR AI v7.0 STARTED</b>

<b>Time:</b> {datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}
<b>Environment:</b> {os.getenv('ENVIRONMENT', 'production').upper()}
<b>Mode:</b> {'ADVISORY' if ADVISORY_MODE else 'TRADING'}
<b>Tracked Symbols:</b> {len(DEFAULT_TRACKED_SYMBOLS)}

System online and monitoring markets 24/7
            """
            
            self.send_message(message.strip())
            
        except Exception as e:
            logger.error(f"Error sending startup notification: {e}")
    
    def send_daily_summary(self, stats: Dict[str, Any]):
        """Send daily performance summary"""
        try:
            message = f"""
ğŸ“Š <b>DAILY SUMMARY</b>

<b>Date:</b> {datetime.now(pytz.UTC).strftime('%Y-%m-%d')}

<b>Signals Generated:</b> {stats.get('signals_today', 0)}
<b>Opportunities Found:</b> {stats.get('opportunities_today', 0)}
<b>Win Rate:</b> {stats.get('win_rate', 0):.1f}%
<b>Total P&L:</b> ${stats.get('total_pnl', 0):.2f}

<b>System Uptime:</b> {stats.get('uptime_hours', 0):.1f}h
<b>API Calls:</b> {stats.get('api_calls', 0):,}

All systems operational âœ…
            """
            
            self.send_message(message.strip())
            
        except Exception as e:
            logger.error(f"Error sending daily summary: {e}")

# Initialize Telegram notifier
telegram_notifier = TelegramNotifier(
    token=TELEGRAM_TOKEN,
    chat_id=TELEGRAM_CHAT_ID
) if TELEGRAM_TOKEN and TELEGRAM_CHAT_ID else None

# ============================================================================
# SYSTEM INITIALIZATION
# ============================================================================

def initialize_all_services():
    """
    Initialize all system services
    This is the MASTER INITIALIZATION function
    """
    try:
        logger.info("=" * 100)
        logger.info("ğŸ”§ INITIALIZING ALL SERVICES")
        logger.info("=" * 100)
        
        init_start = time.time()
        
        # 1. Data Validator (FIRST - validates everything else)
        logger.info("\n[1/12] Initializing Data Validator...")
        state.validator = EnterpriseDataValidator()
        logger.info("âœ… Data Validator ready - ZERO TOLERANCE MODE")
        
        # 2. Database Manager
        logger.info("\n[2/12] Initializing Database Manager...")
        state.db_manager = EnterpriseDatabaseManager(
            db_url=DATABASE_URL,
            min_conn=5,
            max_conn=20
        )
        logger.info("âœ… Database Manager ready")
        
        # 3. Redis Cache (optional)
        logger.info("\n[3/12] Initializing Redis Cache...")
        redis_url = os.getenv('REDIS_URL')
        if redis_url:
            try:
                state.redis_cache = RedisCache(redis_url)
                logger.info("âœ… Redis Cache connected")
            except Exception as e:
                logger.warning(f"âš ï¸  Redis Cache unavailable: {e}")
                state.redis_cache = None
        else:
            logger.warning("âš ï¸  Redis URL not configured - caching disabled")
            state.redis_cache = None
        
        # 4. Multi-Exchange Data Fetcher
        logger.info("\n[4/12] Initializing Multi-Exchange Data Fetcher...")
        state.data_fetcher = MultiExchangeDataFetcher()
        logger.info("âœ… Data Fetcher ready (Binance, Bybit, Coinbase)")
        
        # 5. WebSocket Manager
        logger.info("\n[5/12] Initializing WebSocket Manager...")
        state.ws_manager = BinanceWebSocketManager()
        state.ws_manager.start()
        logger.info("âœ… WebSocket Manager started")
        
        # 6. Signal Orchestrator
        logger.info("\n[6/12] Initializing Signal Orchestrator...")
        state.signal_orchestrator = MasterSignalOrchestrator(
            db_manager=state.db_manager,
            data_fetcher=state.data_fetcher,
            validator=state.validator
        )
        logger.info("âœ… Signal Orchestrator ready (50+ layers)")
        
        # 7. Opportunity Engine
        logger.info("\n[7/12] Initializing Opportunity Engine...")
        state.opportunity_engine = OpportunityEngine(
            db_manager=state.db_manager
        )
        logger.info("âœ… Opportunity Engine ready")
        
        # 8. Advisor Service
        logger.info("\n[8/12] Initializing Advisor Service...")
        state.advisor_service = AdvisorOpportunityService(
            db_manager=state.db_manager
        )
        logger.info("âœ… Advisor Service ready")
        
        # 9. Performance Engine
        logger.info("\n[9/12] Initializing Performance Engine...")
        state.performance_engine = PerformanceEngine(
            db_manager=state.db_manager
        )
        logger.info("âœ… Performance Engine ready")
        
        # 10. Health Monitor
        logger.info("\n[10/12] Initializing Health Monitor...")
        state.health_monitor = HealthMonitor(
            components={
                'database': state.db_manager,
                'websocket': state.ws_manager,
                'data_fetcher': state.data_fetcher
            }
        )
        logger.info("âœ… Health Monitor ready")
        
        # 11. Background Tasks
        logger.info("\n[11/12] Starting Background Tasks...")
        
        # Signal generation every 5 minutes
        task_scheduler.start_task(
            name='signal_generation',
            func=signal_generation_task,
            interval=300  # 5 minutes
        )
        
        # Performance tracking every 1 minute
        task_scheduler.start_task(
            name='performance_tracking',
            func=performance_tracking_task,
            interval=60  # 1 minute
        )
        
        # Health monitoring every 2 minutes
        task_scheduler.start_task(
            name='health_monitoring',
            func=health_monitoring_task,
            interval=120  # 2 minutes
        )
        
        # Database cleanup every 24 hours
        task_scheduler.start_task(
            name='database_cleanup',
            func=database_cleanup_task,
            interval=86400  # 24 hours
        )
        
        logger.info("âœ… Background tasks started")
        
        # 12. Telegram Notifications
        logger.info("\n[12/12] Initializing Telegram Notifications...")
        if telegram_notifier and telegram_notifier.enabled:
            telegram_notifier.send_startup_notification()
            logger.info("âœ… Telegram notifications enabled")
        else:
            logger.warning("âš ï¸  Telegram notifications disabled")
        
        # Mark system as running
        state.is_running = True
        state.start_time = datetime.now()
        
        init_duration = time.time() - init_start
        
        logger.info("=" * 100)
        logger.info(f"âœ…âœ…âœ… ALL SERVICES INITIALIZED SUCCESSFULLY ({init_duration:.2f}s)")
        logger.info("=" * 100)
        
        return True
        
    except Exception as e:
        logger.error("=" * 100)
        logger.error(f"âŒâŒâŒ INITIALIZATION FAILED: {e}")
        logger.error("=" * 100)
        logger.error(traceback.format_exc())
        
        # Send error notification
        if telegram_notifier:
            telegram_notifier.send_error_alert(str(e), 'INITIALIZATION')
        
        return False

# ============================================================================
# GRACEFUL SHUTDOWN
# ============================================================================

def shutdown_handler(signum, frame):
    """
    Graceful shutdown handler
    Ensures clean exit and resource cleanup
    """
    logger.info("\n" + "=" * 100)
    logger.info("ğŸ›‘ SHUTDOWN SIGNAL RECEIVED")
    logger.info("=" * 100)
    
    state.shutdown_requested = True
    
    try:
        # 1. Stop accepting new requests
        logger.info("[1/6] Stopping background tasks...")
        task_scheduler.stop_all()
        
        # 2. Close WebSocket connections
        logger.info("[2/6] Closing WebSocket connections...")
        if state.ws_manager:
            state.ws_manager.stop()
        
        # 3. Save final metrics
        logger.info("[3/6] Saving final metrics...")
        if state.performance_engine:
            final_metrics = state.performance_engine.get_current_metrics()
            logger.info(f"Final stats: {final_metrics}")
        
        # 4. Close database connections
        logger.info("[4/6] Closing database connections...")
        if state.db_manager:
            state.db_manager.close()
        
        # 5. Close Redis connections
        logger.info("[5/6] Closing Redis connections...")
        if state.redis_cache:
            state.redis_cache.close()
        
        # 6. Send shutdown notification
        logger.info("[6/6] Sending shutdown notification...")
        if telegram_notifier:
            telegram_notifier.send_message(
                f"ğŸ›‘ <b>SYSTEM SHUTDOWN</b>\n\n"
                f"Time: {datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}\n"
                f"Uptime: {(datetime.now() - state.start_time).total_seconds() / 3600:.1f}h\n\n"
                f"All services stopped gracefully"
            )
        
        logger.info("=" * 100)
        logger.info("âœ… SHUTDOWN COMPLETE")
        logger.info("=" * 100)
        
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ Error during shutdown: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)

# Register shutdown handlers
signal.signal(signal.SIGINT, shutdown_handler)   # Ctrl+C
signal.signal(signal.SIGTERM, shutdown_handler)  # Kill command

# ============================================================================
# INITIAL SIGNAL GENERATION (On Startup)
# ============================================================================

def generate_initial_signals():
    """
    Generate initial signals on startup
    This populates the database with fresh signals
    """
    try:
        logger.info("\n" + "=" * 100)
        logger.info("ğŸ¯ GENERATING INITIAL SIGNALS")
        logger.info("=" * 100)
        
        if not state.signal_orchestrator:
            logger.error("âŒ Signal orchestrator not initialized")
            return
        
        # Generate signals for all tracked symbols
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        signals = loop.run_until_complete(
            state.signal_orchestrator.process_all_symbols(DEFAULT_TRACKED_SYMBOLS)
        )
        
        loop.close()
        
        logger.info("=" * 100)
        logger.info(f"âœ… INITIAL SIGNALS GENERATED: {len(signals)} signals")
        logger.info("=" * 100 + "\n")
        
        # Send summary to Telegram
        if telegram_notifier and signals:
            telegram_notifier.send_message(
                f"ğŸ¯ <b>INITIAL SCAN COMPLETE</b>\n\n"
                f"Generated {len(signals)} signals for {len(DEFAULT_TRACKED_SYMBOLS)} symbols\n\n"
                f"Dashboard ready for monitoring"
            )
        
    except Exception as e:
        logger.error(f"âŒ Initial signal generation failed: {e}")
        logger.error(traceback.format_exc())

# ============================================================================
# HEALTH CHECK ENDPOINT (Async)
# ============================================================================

@app.route('/api/health/deep', methods=['GET'])
@limiter.limit("10 per minute")
def deep_health_check():
    """
    Deep health check - tests all critical components
    Used for comprehensive monitoring
    """
    try:
        results = {
            'timestamp': datetime.now(pytz.UTC).isoformat(),
            'overall': 'healthy',
            'checks': {}
        }
        
        # Database check
        try:
            if state.db_manager:
                db_healthy = state.db_manager.is_healthy()
                results['checks']['database'] = {
                    'status': 'healthy' if db_healthy else 'unhealthy',
                    'latency_ms': 0  # Would measure actual latency
                }
            else:
                results['checks']['database'] = {'status': 'not_initialized'}
        except Exception as e:
            results['checks']['database'] = {'status': 'error', 'error': str(e)}
            results['overall'] = 'degraded'
        
        # WebSocket check
        try:
            if state.ws_manager:
                ws_healthy = state.ws_manager.is_healthy()
                results['checks']['websocket'] = {
                    'status': 'healthy' if ws_healthy else 'unhealthy',
                    'connected': state.ws_manager.is_connected,
                    'messages_received': state.ws_manager.metrics['messages_received']
                }
            else:
                results['checks']['websocket'] = {'status': 'not_initialized'}
        except Exception as e:
            results['checks']['websocket'] = {'status': 'error', 'error': str(e)}
            results['overall'] = 'degraded'
        
        # API check (Binance)
        try:
            if state.data_fetcher:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                start = time.time()
                price, source = loop.run_until_complete(
                    state.data_fetcher.get_price_with_fallback('BTCUSDT')
                )
                latency = (time.time() - start) * 1000
                
                loop.close()
                
                results['checks']['api'] = {
                    'status': 'healthy',
                    'latency_ms': round(latency, 2),
                    'source': source
                }
            else:
                results['checks']['api'] = {'status': 'not_initialized'}
        except Exception as e:
            results['checks']['api'] = {'status': 'error', 'error': str(e)}
            results['overall'] = 'degraded'
        
        # Background tasks check
        try:
            task_status = task_scheduler.get_status()
            running_tasks = sum(1 for t in task_status.values() if t['status'] == 'running')
            
            results['checks']['background_tasks'] = {
                'status': 'healthy' if running_tasks > 0 else 'unhealthy',
                'running': running_tasks,
                'total': len(task_status)
            }
        except Exception as e:
            results['checks']['background_tasks'] = {'status': 'error', 'error': str(e)}
        
        # Overall status
        unhealthy_checks = sum(
            1 for check in results['checks'].values()
            if check.get('status') not in ['healthy', 'not_initialized']
        )
        
        if unhealthy_checks > 0:
            results['overall'] = 'degraded'
        
        status_code = 200 if results['overall'] == 'healthy' else 503
        
        return jsonify(results), status_code
        
    except Exception as e:
        logger.error(f"Deep health check error: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e),
            'timestamp': datetime.now(pytz.UTC).isoformat()
        }), 500

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Main execution function
    This is where everything starts
    """
    try:
        # Print startup banner
        print_startup_banner()
        
        # Validate environment
        logger.info("ğŸ” Validating environment...")
        
        required_env = ['DATABASE_URL']
        missing = [var for var in required_env if not os.getenv(var)]
        
        if missing:
            logger.error(f"âŒ Missing required environment variables: {', '.join(missing)}")
            sys.exit(1)
        
        logger.info("âœ… Environment validated")
        
        # Initialize all services
        logger.info("\nğŸš€ Starting initialization sequence...")
        
        if not initialize_all_services():
            logger.error("âŒ Initialization failed - exiting")
            sys.exit(1)
        
        # Generate initial signals
        logger.info("\nğŸ¯ Running initial signal generation...")
        generate_initial_signals()
        
        # Get port from environment (Railway)
        port = int(os.getenv('PORT', 8501))
        host = '0.0.0.0'
        
        logger.info("\n" + "=" * 100)
        logger.info(f"ğŸŒ STARTING FLASK SERVER")
        logger.info(f"   Host: {host}")
        logger.info(f"   Port: {port}")
        logger.info(f"   URL: http://{host}:{port}")
        logger.info("=" * 100 + "\n")
        
        # Start Flask server
        app.run(
            host=host,
            port=port,
            debug=False,
            threaded=True,
            use_reloader=False
        )
        
    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ Keyboard interrupt received")
        shutdown_handler(None, None)
        
    except Exception as e:
        logger.error(f"\nâŒ FATAL ERROR: {e}")
        logger.error(traceback.format_exc())
        
        # Send critical error notification
        if telegram_notifier:
            telegram_notifier.send_error_alert(str(e), 'MAIN')
        
        sys.exit(1)

# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == '__main__':
    """
    Application entry point
    """
    try:
        # Set process title (for monitoring)
        try:
            import setproctitle
            setproctitle.setproctitle('demir_ai_v7_master')
        except ImportError:
            pass
        
        # Run main application
        main()
        
    except Exception as e:
        print(f"\nâŒ CRITICAL STARTUP ERROR: {e}")
        print(traceback.format_exc())
        sys.exit(1)

# ============================================================================
# END OF MAIN.PY
# ============================================================================

"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‰ DEMIR AI v7.0 ENTERPRISE - MAIN ORCHESTRATOR COMPLETE

FEATURES IMPLEMENTED:
    âœ… Professional logging system with colors and emojis
    âœ… Global state management (thread-safe singleton)
    âœ… Enterprise data validator (ZERO TOLERANCE for mock/fake data)
    âœ… Performance monitoring and profiling
    âœ… Enterprise database manager with connection pooling
    âœ… Master signal orchestrator (50+ layers)
    âœ… Comprehensive REST API (20+ endpoints)
    âœ… Rate limiting and security
    âœ… Background task scheduler
    âœ… Telegram integration with rich notifications
    âœ… Graceful shutdown handler
    âœ… Deep health checks
    âœ… Initial signal generation on startup
    âœ… Full error handling and logging

DEPLOYMENT READY:
    âœ… Railway
    âœ… Docker
    âœ… Kubernetes
    âœ… Heroku
    âœ… AWS/GCP/Azure

API ENDPOINTS:
    GET  /                          - Dashboard HTML
    GET  /health                    - Health check
    GET  /api/status                - System status
    GET  /api/prices/current        - Current prices
    GET  /api/signals/latest        - Latest signals
    GET  /api/signals/consensus     - Generate consensus signal
    POST /api/signals/generate      - Generate multiple signals
    GET  /api/opportunities         - Trade opportunities (ADVISORY)
    GET  /api/performance/current   - Performance metrics
    POST /api/backtest              - Run backtest
    GET  /api/layers/scores         - Layer contribution analysis
    GET  /api/health/deep           - Deep health check
    GET  /api/debug/validation      - Validation statistics

BACKGROUND TASKS:
    - Signal generation (every 5 minutes)
    - Performance tracking (every 1 minute)
    - Health monitoring (every 2 minutes)
    - Database cleanup (every 24 hours)

SECURITY:
    - Rate limiting
    - CORS protection
    - Request size limits
    - Data validation
    - Error sanitization

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
